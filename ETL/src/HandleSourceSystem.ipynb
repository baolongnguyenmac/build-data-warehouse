{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b78c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pymssql\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as sparkFunction\n",
    "import pyspark.sql.types as dataType\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6863d19",
   "metadata": {},
   "source": [
    "## Connect to db using pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccacd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure something\n",
    "HOSTNAME = 'localhost'\n",
    "PORT = 1433\n",
    "USERNAME = 'sa'\n",
    "PASSWORD = 'Longhandsome123'\n",
    "SOURCE = 'SourceSystem'\n",
    "WAREHOUSE = 'DataStore'\n",
    "METADB = 'MetaDatabase'\n",
    "\n",
    "def createUrl(dbName, hostname=HOSTNAME, port=PORT, username=USERNAME, password=PASSWORD):\n",
    "    return f'jdbc:sqlserver://{hostname}:{port};database={dbName};user={username};password={password}'\n",
    "\n",
    "def shape(df):\n",
    "    print(f'shape: ({df.count()}, {len(df.columns)})')\n",
    "\n",
    "def getSourceFromFile(fileNumber):\n",
    "    dayNum = fileNumber\n",
    "    fileName = f'../Raw Data/customer_order_{dayNum}.csv'\n",
    "\n",
    "    df = spark.read.csv(fileName, header=True, inferSchema=True)\n",
    "\n",
    "    def convertToDateType(dateString):\n",
    "        return datetime.strptime(dateString, '%m/%d/%y')\n",
    "\n",
    "    str2Date = sparkFunction.udf(f=convertToDateType, returnType=dataType.DateType())\n",
    "    df = df.withColumn('Ship Date', str2Date('Ship Date'))\n",
    "    df = df.withColumn('Order Date', str2Date('Order Date'))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1f8309",
   "metadata": {},
   "source": [
    "## Connect to db using pymssql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed14121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect and query in werehouse\n",
    "conn = pymssql.connect(server=HOSTNAME, user=USERNAME, password=PASSWORD, database=WAREHOUSE)\n",
    "conn.autocommit(True)\n",
    "cur = conn.cursor()\n",
    "\n",
    "# connect and query in metaDB\n",
    "conn1 = pymssql.connect(server=HOSTNAME, user=USERNAME, password=PASSWORD, database=METADB)\n",
    "conn1.autocommit(True)\n",
    "cur1 = conn1.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491fdd03",
   "metadata": {},
   "source": [
    "## ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c468072a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ETL:\n",
    "    def __init__(self) -> None:\n",
    "        self.oldRowID = 0\n",
    "        self.newRowID = 0\n",
    "        self._logToMetaDB(0, 'start')\n",
    "        pass\n",
    "\n",
    "    def _getSource(self):\n",
    "        # get max rowID of the previous extraction and assign to oldRowID\n",
    "        queryMeta = f'''(\n",
    "            SELECT a1.rowID\n",
    "            FROM History a1\n",
    "            WHERE NOT EXISTS (\n",
    "                SELECT *\n",
    "                FROM History a2\n",
    "                WHERE a2.rowID > a1.rowID\n",
    "            )\n",
    "        ) T'''\n",
    "        df_meta = spark.read.jdbc(url=createUrl(dbName=METADB), table=queryMeta)\n",
    "\n",
    "        try:\n",
    "            self.oldRowID = df_meta.collect()[0][0]\n",
    "        except:\n",
    "            self.oldRowID = 0\n",
    "\n",
    "        # get all of records whose rowID > oldRowID\n",
    "        print(f'ETL from {self.oldRowID + 1}')\n",
    "        querySource = f'''(\n",
    "            SELECT * \n",
    "            FROM customer_order\n",
    "            WHERE [Row ID] > {self.oldRowID}\n",
    "        ) T'''\n",
    "\n",
    "        return spark.read.jdbc(url=createUrl(dbName=SOURCE), table=querySource)\n",
    "\n",
    "    def _logToMetaDB(self, rowID, detail):\n",
    "        query = None\n",
    "        if rowID is None:\n",
    "            query = f'''\n",
    "                INSERT INTO History (detail) VALUES ('{detail}')\n",
    "            '''\n",
    "        else:\n",
    "            query = f'''\n",
    "                INSERT INTO History (rowID, detail) VALUES ({rowID}, '{detail}')\n",
    "            '''\n",
    "        try:\n",
    "            cur1.execute(query)\n",
    "        except:\n",
    "            print(f'error in _logToMetaDB: rowID: {rowID}, detail: {detail}')\n",
    "\n",
    "    # get new data by: source left join dim\n",
    "    # cols can be conflict --> have to rename\n",
    "    def _getNewData(self, sourceDf, dimDf, cols, idCols, getUpdate=True):\n",
    "        # change colName (otherwise: conflict col_name)\n",
    "        for col in cols:\n",
    "            dimDf = dimDf.withColumnRenamed(col, f'{col}_dim')\n",
    "\n",
    "        # source left join dim and return new data\n",
    "        joinData = sourceDf.join(dimDf, on=idCols, how='left')\n",
    "\n",
    "        updateData = None\n",
    "        # if we're not extracting dimDate\n",
    "        if getUpdate:\n",
    "            updateData = joinData.where(joinData[f'{cols[0]}_dim'].isNotNull())\n",
    "            # get the max shipDate and min orderDate and update\n",
    "            rowMax = sparkFunction.greatest(updateData['ID Expiry Date'], updateData['ID Expiry Date_dim'])\n",
    "            rowMin = sparkFunction.least(updateData['ID Manufacturing Date'], updateData['ID Manufacturing Date_dim'])\n",
    "            updateData = updateData.withColumn('ID Manufacturing Date', rowMin)\n",
    "            updateData = updateData.withColumn('ID Expiry Date', rowMax)\n",
    "            updateData = updateData.drop(*[f'{col}_dim' for col in cols])\n",
    "            if updateData.count() == 0:\n",
    "                updateData = None\n",
    "\n",
    "        newData = joinData.where(joinData[f'{cols[0]}_dim'].isNull())\n",
    "        # drop all the null value in the right side\n",
    "        newData = newData.drop(*[f'{col}_dim' for col in cols])\n",
    "\n",
    "        return newData, updateData\n",
    "\n",
    "    def _getSourceSystemDate(self, df):\n",
    "        shipDate = df[['Ship Date']]\n",
    "        orderDate = df[['Order Date']]\n",
    "        orderDate = orderDate.withColumnRenamed('Order Date', 'Ship Date')\n",
    "\n",
    "        date = shipDate.union(orderDate)\n",
    "        date = date.withColumnRenamed('Ship Date', 'date')\n",
    "\n",
    "        # drop duplicate and make id\n",
    "        date = date.dropDuplicates()\n",
    "        date = date.withColumn('idDate', sparkFunction.date_format(date['date'], 'yyyyMMdd'))\n",
    "        date = date.withColumn('idDate', date['idDate'].cast('int'))\n",
    "\n",
    "        return date\n",
    "\n",
    "    def ETLDate(self, df):\n",
    "        sourceDate = self._getSourceSystemDate(df)\n",
    "        dimDate = spark.read.jdbc(url=createUrl(dbName=WAREHOUSE), table='dimDate')\n",
    "\n",
    "        if dimDate.count() == 0:\n",
    "            numOfNewDate = sourceDate.count()\n",
    "            if numOfNewDate != 0:\n",
    "                sourceDate.write.jdbc(url=createUrl(dbName=WAREHOUSE), table='dimDate', mode='append')\n",
    "                # self.newRowID = self.oldRowID + numOfNewDate\n",
    "                stt = f'Write {numOfNewDate} dates into db'\n",
    "                print(stt)\n",
    "                self._logToMetaDB(self.newRowID, stt)\n",
    "            else:\n",
    "                print('nothing new in source date')\n",
    "        else:\n",
    "            newDate, _ = self._getNewData(sourceDate, dimDate, ['date'], 'idDate', getUpdate=False)\n",
    "            numOfNewDate = newDate.count()\n",
    "            if numOfNewDate != 0:\n",
    "                # self.newRowID = self.oldRowID + numOfNewDate\n",
    "                newDate.write.jdbc(url=createUrl(dbName=WAREHOUSE), table='dimDate', mode='append')\n",
    "                stt = f'Write {numOfNewDate} dates into db'\n",
    "                print(stt)\n",
    "                self._logToMetaDB(self.newRowID, stt)\n",
    "\n",
    "    def _getSourceSystemCustomer(self, df):\n",
    "        sourceCustomer = df.groupBy(['Customer ID', 'Customer Name', 'Segment', 'Country', 'City', 'State', 'Postal Code', 'Region']).agg({'Order Date': 'min', 'Ship Date': 'max'})\n",
    "        sourceCustomer = sourceCustomer.withColumnRenamed('max(Ship Date)', 'ID Expiry Date')\n",
    "        sourceCustomer = sourceCustomer.withColumnRenamed('min(Order Date)', 'ID Manufacturing Date')\n",
    "        sourceCustomer = sourceCustomer.withColumn('ID Manufacturing Date', \n",
    "                                                    sparkFunction.date_format(sourceCustomer['ID Manufacturing Date'], 'yyyyMMdd'))\n",
    "        sourceCustomer = sourceCustomer.withColumn('ID Manufacturing Date', \n",
    "                                                    sourceCustomer['ID Manufacturing Date'].cast('int'))\n",
    "        sourceCustomer = sourceCustomer.withColumn('ID Expiry Date', \n",
    "                                                    sparkFunction.date_format(sourceCustomer['ID Expiry Date'], 'yyyyMMdd'))\n",
    "        sourceCustomer = sourceCustomer.withColumn('ID Expiry Date', \n",
    "                                                    sourceCustomer['ID Expiry Date'].cast('int'))\n",
    "        return sourceCustomer\n",
    "\n",
    "    def _handleUpdatedDataCustomer(self, df):\n",
    "        l = df.collect()\n",
    "        for i in l:\n",
    "            query = f'''\n",
    "                MERGE INTO dimCustomer AS t\n",
    "                USING \n",
    "                    (SELECT \n",
    "                        [Customer ID] = '{i['Customer ID']}', \n",
    "                        [Customer Name] = '{i['Customer Name'].replace(\"'\", '\"')}',\n",
    "                        [Segment] = '{i['Segment'].replace(\"'\", '\"')}',\n",
    "                        [Country] = '{i['Country'].replace(\"'\", '\"')}',\n",
    "                        [City] = '{i['City'].replace(\"'\", '\"')}',\n",
    "                        [State] = '{i['State'].replace(\"'\", '\"')}',\n",
    "                        [Postal Code] = {i['Postal Code']},\n",
    "                        [Region] = '{i['Region'].replace(\"'\", '\"')}',\n",
    "                        [ID Manufacturing Date] = {i['ID Manufacturing Date']},\n",
    "                        [ID Expiry Date] = {i['ID Expiry Date']},\n",
    "                        [idCustomer] = {i['idCustomer']}) AS s\n",
    "                ON t.idCustomer = s.idCustomer\n",
    "                WHEN MATCHED THEN UPDATE SET \n",
    "                        [Customer ID] = s.[Customer ID],\n",
    "                        [Customer Name] = s.[Customer Name],\n",
    "                        [Segment] = s.[Segment],\n",
    "                        [Country] = s.[Country],\n",
    "                        [City] = s.[City],\n",
    "                        [State] = s.[State],\n",
    "                        [Postal Code] = s.[Postal Code],\n",
    "                        [Region] = s.[Region],\n",
    "                        [ID Manufacturing Date] =  s.[ID Manufacturing Date],\n",
    "                        [ID Expiry Date] =  s.[ID Expiry Date];\n",
    "            '''\n",
    "            try:\n",
    "                cur.execute(query)\n",
    "                # conn.commit()\n",
    "            except:\n",
    "                print(f'error in handleUpdatedDataCustomer, update failed {len(l)} customers')\n",
    "                return\n",
    "        stt = f'Updated {len(l)} customers'\n",
    "        print(stt)\n",
    "        self._logToMetaDB(self.newRowID, stt)\n",
    "\n",
    "    def ETLCustomer(self, df):\n",
    "        sourceCustomer = self._getSourceSystemCustomer(df)\n",
    "        dimCustomer = spark.read.jdbc(url=createUrl(dbName=WAREHOUSE), table='dimCustomer')\n",
    "\n",
    "        if dimCustomer.count() == 0:\n",
    "            numOfNewCustomer = sourceCustomer.count()\n",
    "            if numOfNewCustomer != 0:\n",
    "                sourceCustomer.write.jdbc(url=createUrl(dbName=WAREHOUSE), table='dimCustomer', mode='append')\n",
    "                stt = f'Write {numOfNewCustomer} customer into db'\n",
    "                print(stt)\n",
    "                self._logToMetaDB(self.newRowID, stt)\n",
    "            else:\n",
    "                print('nothing new in source customer')\n",
    "        else:\n",
    "            newCustomer, updateCustomer = self._getNewData(\n",
    "                sourceCustomer, \n",
    "                dimCustomer, \n",
    "                ['ID Manufacturing Date', 'ID Expiry Date'], \n",
    "                ['Customer ID', 'Customer Name', 'Segment', 'Country', 'City', 'State', 'Postal Code', 'Region'])\n",
    "            if updateCustomer is not None:\n",
    "                self._handleUpdatedDataCustomer(updateCustomer)\n",
    "            numOfNewCustomer = newCustomer.count()\n",
    "            if numOfNewCustomer != 0:\n",
    "                newCustomer = newCustomer.drop('idCustomer')\n",
    "                newCustomer.write.jdbc(url=createUrl(dbName=WAREHOUSE), table='dimCustomer', mode='append')\n",
    "                stt = f'Write {numOfNewCustomer} customer into db'\n",
    "                print(stt)\n",
    "                self._logToMetaDB(self.newRowID, stt)\n",
    "            else:\n",
    "                print('nothing new in source customer')\n",
    "\n",
    "    def _getSourceSystemProduct(self, df):\n",
    "        sourceProduct = df.groupBy(['Product ID', 'Product Name', 'Category', 'Sub-Category']).agg({'Order Date': 'min', 'Ship Date': 'max'})\n",
    "        sourceProduct = sourceProduct.withColumnRenamed('max(Ship Date)', 'ID Expiry Date')\n",
    "        sourceProduct = sourceProduct.withColumnRenamed('min(Order Date)', 'ID Manufacturing Date')\n",
    "        sourceProduct = sourceProduct.withColumn('ID Manufacturing Date', \n",
    "                                                sparkFunction.date_format(sourceProduct['ID Manufacturing Date'], 'yyyyMMdd'))\n",
    "        sourceProduct = sourceProduct.withColumn('ID Manufacturing Date', \n",
    "                                                sourceProduct['ID Manufacturing Date'].cast('int'))\n",
    "        sourceProduct = sourceProduct.withColumn('ID Expiry Date', \n",
    "                                                sparkFunction.date_format(sourceProduct['ID Expiry Date'], 'yyyyMMdd'))\n",
    "        sourceProduct = sourceProduct.withColumn('ID Expiry Date', \n",
    "                                                sourceProduct['ID Expiry Date'].cast('int'))\n",
    "        return sourceProduct\n",
    "\n",
    "    def _handleUpdatedDataProduct(self, df):\n",
    "        l = df.collect()\n",
    "        for i in l:\n",
    "            query = f'''\n",
    "                MERGE INTO dimProduct AS t\n",
    "                USING \n",
    "                    (SELECT \n",
    "                        [idProduct] = {i['idProduct']},\n",
    "                        [Product ID] = '{i['Product ID']}',\n",
    "                        [Product Name] = '{i['Product Name'].replace(\"'\", '\"')}',\n",
    "                        [Category] = '{i['Category'].replace(\"'\", '\"')}',\n",
    "                        [Sub-Category] = '{i['Sub-Category'].replace(\"'\", '\"')}',\n",
    "                        [ID Manufacturing Date] = {i['ID Manufacturing Date']},\n",
    "                        [ID Expiry Date] = {i['ID Expiry Date']}) AS s\n",
    "                ON t.idProduct = s.idProduct\n",
    "                WHEN MATCHED THEN UPDATE SET \n",
    "                        [Product ID] = s.[Product ID],\n",
    "                        [Product Name] = s.[Product Name],\n",
    "                        [Category] = s.[Category],\n",
    "                        [Sub-Category] = s.[Sub-Category],\n",
    "                        [ID Manufacturing Date] = s.[ID Manufacturing Date],\n",
    "                        [ID Expiry Date] = s.[ID Expiry Date];\n",
    "            '''\n",
    "            try:\n",
    "                cur.execute(query)\n",
    "                # conn.commit()\n",
    "            except:\n",
    "                print(f'error in handleUpdatedDataProduct, update failed {len(l)} products')\n",
    "                return\n",
    "        stt = f'Updated {len(l)} products'\n",
    "        print(stt)\n",
    "        self._logToMetaDB(self.newRowID, stt)\n",
    "\n",
    "    def ETLProduct(self, df):\n",
    "        sourceProduct = self._getSourceSystemProduct(df)\n",
    "        dimProduct = spark.read.jdbc(url=createUrl(dbName=WAREHOUSE), table='dimProduct')\n",
    "\n",
    "        if dimProduct.count() == 0:\n",
    "            numOfNewProduct = sourceProduct.count()\n",
    "            if numOfNewProduct != 0:\n",
    "                sourceProduct.write.jdbc(url=createUrl(dbName=WAREHOUSE), table='dimProduct', mode='append')\n",
    "                stt = f'Write {numOfNewProduct} products into db'\n",
    "                print(stt)\n",
    "                self._logToMetaDB(self.newRowID, stt)\n",
    "            else:\n",
    "                print('nothing new in source product')\n",
    "        else:\n",
    "            newProduct, updateProduct = self._getNewData(\n",
    "                sourceProduct, \n",
    "                dimProduct, \n",
    "                ['ID Manufacturing Date', 'ID Expiry Date'], \n",
    "                ['Product ID', 'Product Name', 'Category', 'Sub-Category'])\n",
    "            if updateProduct is not None:\n",
    "                self._handleUpdatedDataProduct(updateProduct)\n",
    "            numOfNewProduct = newProduct.count()\n",
    "            if numOfNewProduct != 0:\n",
    "                newProduct = newProduct.drop('idProduct')\n",
    "                newProduct.write.jdbc(url=createUrl(dbName=WAREHOUSE), table='dimProduct', mode='append')\n",
    "                stt = f'Write {numOfNewProduct} products into db'\n",
    "                print(stt)\n",
    "                self._logToMetaDB(self.newRowID, stt)\n",
    "            else:\n",
    "                print('nothing new in source product')\n",
    "\n",
    "    def _getSourceSystemSale(self, df):\n",
    "        sourceSale = df[['Row ID', 'Order ID', 'Order Date', 'Ship Date', 'Ship Mode', 'Sales', 'Quantity', 'Discount', 'Profit', \n",
    "                        'Product ID', 'Product Name', 'Category', 'Sub-Category', \n",
    "                        'Customer ID', 'Customer Name', 'Segment', 'Country', 'City', 'State', 'Postal Code', 'Region']]\n",
    "        sourceSale = sourceSale.withColumn('ID Order Date', sparkFunction.date_format(sourceSale['Order Date'], 'yyyyMMdd'))\n",
    "        sourceSale = sourceSale.withColumn('ID Ship Date', sparkFunction.date_format(sourceSale['Ship Date'], 'yyyyMMdd'))\n",
    "        sourceSale = sourceSale.drop('Ship Date', 'Order Date')\n",
    "\n",
    "        dimProduct = spark.read.jdbc(url=createUrl(dbName=WAREHOUSE), table='dimProduct')\n",
    "        sourceSale = sourceSale.join(dimProduct, on=['Product ID', 'Product Name', 'Category', 'Sub-Category'], how='inner')\n",
    "        sourceSale = sourceSale.drop('Product ID', 'Product Name', 'Category', 'Sub-Category', 'ID Manufacturing Date', 'ID Expiry Date')\n",
    "        \n",
    "        dimCustomer = spark.read.jdbc(url=createUrl(dbName=WAREHOUSE), table='dimCustomer')\n",
    "        sourceSale = sourceSale.join(dimCustomer, on=['Customer ID', 'Customer Name', 'Segment', 'Country', 'City', 'State', 'Postal Code', 'Region'], how='inner')\n",
    "        sourceSale = sourceSale.drop('Customer ID', 'Customer Name', 'Segment', 'Country', 'City', 'State', 'Postal Code', 'Region', 'ID Manufacturing Date', 'ID Expiry Date')\n",
    "        return sourceSale\n",
    "\n",
    "    def ETLSaleDetail(self, df):\n",
    "        sourceSale = self._getSourceSystemSale(df)\n",
    "\n",
    "        numOfSource = sourceSale.count()\n",
    "        if numOfSource != 0:\n",
    "            sourceSale.write.jdbc(url=createUrl(dbName=WAREHOUSE), table='factSaleDetail', mode='append')\n",
    "            stt = f'Write {numOfSource} records'\n",
    "            print(stt)\n",
    "            self._logToMetaDB(self.newRowID, stt)\n",
    "        else:\n",
    "            print('nothing new in source sale detail')\n",
    "\n",
    "    def doETL(self):\n",
    "        df = self._getSource()\n",
    "        self.newRowID = self.oldRowID + df.count()\n",
    "        self.ETLDate(df)\n",
    "        self.ETLCustomer(df)\n",
    "        self.ETLProduct(df)\n",
    "        self.ETLSaleDetail(df)\n",
    "        stt = 'finish'\n",
    "        self._logToMetaDB(self.newRowID, stt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83b55d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "etler = ETL()\n",
    "etler.doETL()\n",
    "\n",
    "cur.close()\n",
    "conn.close()\n",
    "\n",
    "cur1.close()\n",
    "conn1.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
